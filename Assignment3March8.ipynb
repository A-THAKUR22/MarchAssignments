{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYUsNwWAvT0f2KDFzAtDeq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/A-THAKUR22/MarchAssignments/blob/main/Assignment3March8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_hT3My8nW5l",
        "outputId": "98a5befa-c2ab-4664-a95c-29c310397f01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488493 sha256=0ec608d655e56ada6452f268c478732c43d71a563e8a77b1e6341b7e137ee04a\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the Walmart Stock CSV File, have Spark infer the data types."
      ],
      "metadata": {
        "id": "QZxZ2-_fnXjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName(\"Walmart Analysis\").getOrCreate()\n",
        "stock_df=spark.read.csv(\"walmart_stock.csv\", header=True, inferSchema=True)\n",
        "stock_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8uQI7zspgGM",
        "outputId": "88589628-2611-4947-9145-bb816542a478"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Date: date, Open: double, High: double, Low: double, Close: double, Volume: int, Adj Close: double]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are the column names?"
      ],
      "metadata": {
        "id": "9hNLtl96sLzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOopG_aEsRFt",
        "outputId": "1b786b98-14b6-4e48-ea57-6a088a98032f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What does the Schema look like?"
      ],
      "metadata": {
        "id": "mmSIOFVmsXnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhh_V0NBsayL",
        "outputId": "eede6e87-d2cb-484c-cca5-c7d2ed64c87a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Date: date (nullable = true)\n",
            " |-- Open: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- Volume: integer (nullable = true)\n",
            " |-- Adj Close: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print out the first 5 columns."
      ],
      "metadata": {
        "id": "aDDvuno_t7Bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_5=stock_df.select(stock_df.columns[:5]).show(5)\n",
        "print(first_5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNH05v5BttUb",
        "outputId": "6ea78868-cf65-4f5c-d6e7-25f1264455f6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+---------+---------+------------------+\n",
            "|      Date|              Open|     High|      Low|             Close|\n",
            "+----------+------------------+---------+---------+------------------+\n",
            "|2012-01-03|         59.970001|61.060001|59.869999|         60.330002|\n",
            "|2012-01-04|60.209998999999996|60.349998|59.470001|59.709998999999996|\n",
            "|2012-01-05|         59.349998|59.619999|58.369999|         59.419998|\n",
            "|2012-01-06|         59.419998|59.450001|58.869999|              59.0|\n",
            "|2012-01-09|         59.029999|59.549999|58.919998|             59.18|\n",
            "+----------+------------------+---------+---------+------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use describe() to learn about the DataFrame."
      ],
      "metadata": {
        "id": "0uBCg74zu_NP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock_df.describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wII-E700vDXE",
        "outputId": "b2eebe15-fef1-4667-935c-4c3049b37e90"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "|summary|              Open|             High|              Low|            Close|           Volume|        Adj Close|\n",
            "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "|  count|              1258|             1258|             1258|             1258|             1258|             1258|\n",
            "|   mean| 72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|\n",
            "| stddev|  6.76809024470826|6.768186808159218|6.744075756255496|6.756859163732991|  4519780.8431556|6.722609449996857|\n",
            "|    min|56.389998999999996|        57.060001|        56.299999|        56.419998|          2094900|        50.363689|\n",
            "|    max|         90.800003|        90.970001|            89.25|        90.470001|         80898100|84.91421600000001|\n",
            "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are too many decimal places for mean and stddev in the describe() dataframe. Format the numbers to just show up to two decimal places. Pay careful attention to the datatypes that .describe() returns, we didn't cover how to do this exact formatting, but we covered something very similar.\n"
      ],
      "metadata": {
        "id": "isE76q6GveQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import format_number\n",
        "descr=stock_df.describe()\n",
        "formatted=descr.select(descr['summary'],format_number(descr['Open'].cast('float'),2).alias('Open'),\n",
        "                       format_number(descr['High'].cast('float'),2).alias('High'),\n",
        "                       format_number(descr['Low'].cast('float'),2).alias('Low'),\n",
        "                       format_number(descr['Close'].cast('float'),2).alias('Close'),\n",
        "                       format_number(descr['Volume'].cast('float'),2).alias('Volume')).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3vOdO7Hvm-q",
        "outputId": "9d4b4e0d-8b0a-4eb0-c046-424dcc6f98aa"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+--------+--------+--------+-------------+\n",
            "|summary|    Open|    High|     Low|   Close|       Volume|\n",
            "+-------+--------+--------+--------+--------+-------------+\n",
            "|  count|1,258.00|1,258.00|1,258.00|1,258.00|     1,258.00|\n",
            "|   mean|   72.36|   72.84|   71.92|   72.39| 8,222,093.50|\n",
            "| stddev|    6.77|    6.77|    6.74|    6.76| 4,519,781.00|\n",
            "|    min|   56.39|   57.06|   56.30|   56.42| 2,094,900.00|\n",
            "|    max|   90.80|   90.97|   89.25|   90.47|80,898,096.00|\n",
            "+-------+--------+--------+--------+--------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new dataframe with a column called HV Ratio that is the ratio of the High Price versus volume of stock traded for a day"
      ],
      "metadata": {
        "id": "h2EotNczzBHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "hv_ratio=stock_df.withColumn('HV Ratio',col('High')/col('Volume'))\n",
        "hv_ratio.select(\"HV Ratio\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toOe39LfzHz1",
        "outputId": "f155138a-a06b-4f5f-9981-b414b16c188a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|            HV Ratio|\n",
            "+--------------------+\n",
            "|4.819714653321546E-6|\n",
            "|6.290848613094555E-6|\n",
            "|4.669412994783916E-6|\n",
            "|7.367338463826307E-6|\n",
            "|8.915604778943901E-6|\n",
            "|8.644477436914568E-6|\n",
            "|9.351828421515645E-6|\n",
            "| 8.29141562102703E-6|\n",
            "|7.712212102001476E-6|\n",
            "|7.071764823529412E-6|\n",
            "|1.015495466386981E-5|\n",
            "|6.576354146362592...|\n",
            "| 5.90145296180676E-6|\n",
            "|8.547679455011844E-6|\n",
            "|8.420709512685392E-6|\n",
            "|1.041448341728929...|\n",
            "|8.316075414862431E-6|\n",
            "|9.721183814992126E-6|\n",
            "|8.029436027707578E-6|\n",
            "|6.307432259386365E-6|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What day had the Peak High in Price?"
      ],
      "metadata": {
        "id": "oVL4tnRL1RrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import desc\n",
        "Highest_price_day=stock_df.orderBy(desc(\"High\")).select(\"Date\").first()[0]\n",
        "Highest_price_day"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmtvtRLp1X3j",
        "outputId": "12917b3b-3424-4c36-e41c-104cfdffc083"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datetime.date(2015, 1, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the max and min of the Volume column?"
      ],
      "metadata": {
        "id": "ruhNGWJL3vV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import max,min\n",
        "max_min_vol=stock_df.select(max(\"Volume\"),min(\"Volume\"))\n",
        "max_min_vol.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wP5Mbbtr3v98",
        "outputId": "879060cb-c7ed-42a9-ab1c-390aab3d763b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+\n",
            "|max(Volume)|min(Volume)|\n",
            "+-----------+-----------+\n",
            "|   80898100|    2094900|\n",
            "+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many days was the Close lower than 60 dollars?"
      ],
      "metadata": {
        "id": "YX9Cg1Zi4ZCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "days_lower_close_60=stock_df.filter(col(\"Close\")<60).count()\n",
        "print(days_lower_close_60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UG2g00Zy4l-z",
        "outputId": "6c884488-5540-48cf-ad2f-48bf6a39b212"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What percentage of the time was the High greater than 80 dollars?"
      ],
      "metadata": {
        "id": "91Xg4xbG5mId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "Higher_price_80=stock_df.filter(col(\"High\")>80).count()\n",
        "total=stock_df.count()\n",
        "percentage=(Higher_price_80/total)*100\n",
        "print(round(percentage,6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWttJ42d5qt5",
        "outputId": "51500d78-b1a7-452d-b42a-c7ea333e1b89"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9.141494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the max High per year"
      ],
      "metadata": {
        "id": "S9ky-jui64ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import max,year\n",
        "\n",
        "df_Years=stock_df.withColumn('Year',year(stock_df['Date']))\n",
        "highest_of_all_years=df_Years.groupBy('Year').agg(max('High').alias('MaxHighAllYears'))\n",
        "highest_of_all_years.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0Vz03Eq68Dq",
        "outputId": "4da7cb15-fb85-4f38-9215-74e57c1ab0e5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------------+\n",
            "|Year|MaxHighAllYears|\n",
            "+----+---------------+\n",
            "|2015|      90.970001|\n",
            "|2013|      81.370003|\n",
            "|2014|      88.089996|\n",
            "|2012|      77.599998|\n",
            "|2016|      75.190002|\n",
            "+----+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the Pearson correlation between High and Volume?"
      ],
      "metadata": {
        "id": "X9YvmiAe_jxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import corr\n",
        "correlation_high_vol=stock_df.corr('High','Volume')\n",
        "correlation_high_vol"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnqU1CMx_pH5",
        "outputId": "6a3afc3f-47c8-4796-9be9-cd8a28396b97"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.3384326061737161"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the average Close for each Calendar Month?\n",
        "In other words, across all the years, what is the average Close price for Jan,Feb, Mar, etc... Your result will have a value for each of these months.\n"
      ],
      "metadata": {
        "id": "rNpK1UWKAt6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import month,avg\n",
        "df_month=stock_df.withColumn('Months',month(stock_df['Date']))\n",
        "new=df_month.groupby('Months').agg(avg('Close').alias('Average Closing Price'))\n",
        "monthly_closing_price=new.orderBy('Months')\n",
        "monthly_closing_price.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFbTfIP5BJ77",
        "outputId": "9e0a7d40-2658-454c-fe93-c21ebb4c5d6f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------------------+\n",
            "|Months|Average Closing Price|\n",
            "+------+---------------------+\n",
            "|     1|    71.44801958415842|\n",
            "|     2|      71.306804443299|\n",
            "|     3|    71.77794377570092|\n",
            "|     4|    72.97361900952382|\n",
            "|     5|    72.30971688679247|\n",
            "|     6|     72.4953774245283|\n",
            "|     7|    74.43971943925233|\n",
            "|     8|    73.02981855454546|\n",
            "|     9|    72.18411785294116|\n",
            "|    10|    71.57854545454543|\n",
            "|    11|     72.1110893069307|\n",
            "|    12|    72.84792478301885|\n",
            "+------+---------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}